# Language Constrained Multimodal Hyper Adapter (LCMHA)

This repository contains the official implementation of the paper:

> **Language Constrained Multimodal Hyper Adapter For Many-to-Many Multimodal Summarization**
> *Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (ACL 2025)*

**LCMHA** improves many-to-many multimodal summarization (M3S) by using a **Language Constrained Hypernetwork** to generate language-specific adapter weights. This allows the model to balance shared multimodal knowledge with language-specific patterns, preventing interference between languages.

## üåü Key Features
* **Hypernetwork-based Adapters:** Generates dynamic weights for adapters based on source/target language embeddings.
* **Language Regularization:** A classification loss ensures the hypernetwork learns distinct language representations.
* **Multimodal Fusion:** Integrates visual features (Faster R-CNN) with text using a joint multimodal adapter.


## üõ†Ô∏è Installation

### 1. Environment Setup
**Crucial Step:** This project relies on a **customized version of the transformers library** provided in this repository (located in the `./transformers` folder). You **must** install this local version instead of the official PyPI version to enable the hypernetwork features.

```bash
# 1. Create a conda environment
conda create -n lcmha python=3.9
conda activate lcmha

# 2. Install the LOCAL customized transformers library
# (Ensure you are in the project root directory containing the 'transformers' folder)
cd transformers-4.44.0
pip install -e ./transformers

# 3. Install other dependencies
pip install torch torchvision torchaudio --index-url [https://download.pytorch.org/whl/cu118](https://download.pytorch.org/whl/cu118)
pip install peft rouge_score nltk h5py datasets tqdm
```

### 2. Project Structure
```bash
hparams/: Argument definitions (Model, Data, LoRA, HyperAdapter).

transformers/: Customized Transformers library (Required).

train.py: Main training script.

test.py: Inference and evaluation script.

run_train.sh: Example training script.

run_test.sh: Example inference script.

utils.py: Utility functions.
```

üìÇ Data Preparation

Text Data
Prepare your dataset in a text file. 

* The text data is in our data folder.

* Image Data & Visual Features. Image data can be collected following the SOV-MAS project [images_crawl](https://github.com/XL2248/SOV-MAS/tree/main/images_crawl). 

* For visual feature extraction, we follow the SOV-MAS pipeline as well, specifically: [extract_roi4all_lang.sh](https://github.com/XL2248/SOV-MAS/blob/main/feature_extraction/extract_roi4all_lang.sh).  



üöÄ Training

To train the model (e.g., using LLaMA-3-8B as backbone), first run save_model_bin.py to obtain a stably initialized model containing the hyper structure, and then use the provided run_train.sh script.

Step 1: Initialize Hyper-Structure

Run the initialization script to prepare the model weights:
```bash
python save_model_bin.py \
    --model_name_or_path /path/to/original/llama-3 \
    --new_structural_model_path /path/to/save/init_model \
    --use_hyper true
```

Step 2: Start Training

Use the initialized model path from Step 1 in your training script:

```bash
bash run_train.sh
```

Manual Command:

```bash
python train.py \
    --train_dataset ./data/train.txt \
    --model_name_or_path /path/to/llama-3-8b \
    --output_dir ./output_dir \
    --do_train true \
    --cutoff_len 1024 \
    --per_device_train_batch_size 1 \
    --gradient_accumulation_steps 8 \
    --learning_rate 1.0e-4 \
    --num_train_epochs 3.0 \
    --use_hyper true \
    --lora_train_hyper true \
    --use_img true \
    --hyper_classification true \
    --hyper_classification_loss_ratio 0.4
  ```

üìä Inference & Evaluation

To generate summaries and calculate ROUGE scores, use run_test.sh.
```bash
bash run_test.sh
```
Manual Command:
```bash
CUDA_VISIBLE_DEVICES=0 python test.py \
    --model_name_or_path /path/to/base_model \
    --test_checkpoint ./checkpoint/your_best_checkpoint \
    --test_dataset ./data/test_data/xx-xx.txt \
    --output_prediction_path ./results \
    --use_hyper true \
    --hyper_predict true \
    --use_img true
``` 
The script will automatically compute ROUGE-1, ROUGE-2, and ROUGE-L scores after generation.

üìù Citation

If you use this code or dataset in your work, please cite our ACL 2025 paper:
```
@inproceedings{liu-etal-2025-language,
    title = "Language Constrained Multimodal Hyper Adapter For Many-to-Many Multimodal Summarization",
    author = "Liu, Nayu  and Yao, Fanglong  and Luo, Haoran  and Yang, Yong  and Tang, Chen  and
      Lv, Bo",
      editor = "Che, Wanxiang  andNabende, Joyce  andShutova, Ekaterina  and Pilehvar, Mohammad Taher",
    booktitle = "Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics      (Volume 1: Long Papers)",
    month = jul,
    year = "2025",
    address = "Vienna, Austria",
    publisher = "Association for Computational Linguistics",
    doi = "10.18653/v1/2025.acl-long.1229",
    pages = "25285--25298",
    ISBN = "979-8-89176-251-0",
  
}
```
